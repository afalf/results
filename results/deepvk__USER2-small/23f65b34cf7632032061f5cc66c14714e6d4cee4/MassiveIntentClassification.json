{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "task_name": "MassiveIntentClassification",
  "mteb_version": "1.38.0",
  "scores": {
    "validation": [
      {
        "accuracy": 0.666995,
        "f1": 0.643967,
        "f1_weighted": 0.666747,
        "scores_per_experiment": [
          {
            "accuracy": 0.680767,
            "f1": 0.672953,
            "f1_weighted": 0.681914
          },
          {
            "accuracy": 0.684702,
            "f1": 0.637199,
            "f1_weighted": 0.686631
          },
          {
            "accuracy": 0.666503,
            "f1": 0.648616,
            "f1_weighted": 0.665579
          },
          {
            "accuracy": 0.689621,
            "f1": 0.657123,
            "f1_weighted": 0.685811
          },
          {
            "accuracy": 0.670438,
            "f1": 0.632977,
            "f1_weighted": 0.669339
          },
          {
            "accuracy": 0.64486,
            "f1": 0.634696,
            "f1_weighted": 0.643422
          },
          {
            "accuracy": 0.645844,
            "f1": 0.631151,
            "f1_weighted": 0.647155
          },
          {
            "accuracy": 0.653222,
            "f1": 0.634657,
            "f1_weighted": 0.653055
          },
          {
            "accuracy": 0.663551,
            "f1": 0.644659,
            "f1_weighted": 0.661982
          },
          {
            "accuracy": 0.670438,
            "f1": 0.645636,
            "f1_weighted": 0.672579
          }
        ],
        "main_score": 0.666995,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.660861,
        "f1": 0.636051,
        "f1_weighted": 0.663091,
        "scores_per_experiment": [
          {
            "accuracy": 0.680229,
            "f1": 0.660851,
            "f1_weighted": 0.681357
          },
          {
            "accuracy": 0.677202,
            "f1": 0.63632,
            "f1_weighted": 0.681052
          },
          {
            "accuracy": 0.637525,
            "f1": 0.617713,
            "f1_weighted": 0.642201
          },
          {
            "accuracy": 0.682246,
            "f1": 0.649758,
            "f1_weighted": 0.682883
          },
          {
            "accuracy": 0.661399,
            "f1": 0.634073,
            "f1_weighted": 0.660421
          },
          {
            "accuracy": 0.642569,
            "f1": 0.622346,
            "f1_weighted": 0.645013
          },
          {
            "accuracy": 0.644923,
            "f1": 0.634327,
            "f1_weighted": 0.650587
          },
          {
            "accuracy": 0.659381,
            "f1": 0.628652,
            "f1_weighted": 0.660902
          },
          {
            "accuracy": 0.64963,
            "f1": 0.623935,
            "f1_weighted": 0.650821
          },
          {
            "accuracy": 0.673504,
            "f1": 0.652532,
            "f1_weighted": 0.675676
          }
        ],
        "main_score": 0.660861,
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 34.685758113861084,
  "kg_co2_emissions": null
}