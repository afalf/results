{
  "dataset_revision": "f6d2c31f4dc6b88f468552750bfec05b4b41b05a",
  "task_name": "RuReviewsClassification",
  "mteb_version": "1.38.0",
  "scores": {
    "test": [
      {
        "accuracy": 0.627393,
        "f1": 0.626576,
        "f1_weighted": 0.626592,
        "scores_per_experiment": [
          {
            "accuracy": 0.674316,
            "f1": 0.676814,
            "f1_weighted": 0.676838
          },
          {
            "accuracy": 0.555176,
            "f1": 0.554899,
            "f1_weighted": 0.554939
          },
          {
            "accuracy": 0.623535,
            "f1": 0.629775,
            "f1_weighted": 0.629768
          },
          {
            "accuracy": 0.672363,
            "f1": 0.675029,
            "f1_weighted": 0.675045
          },
          {
            "accuracy": 0.647461,
            "f1": 0.654827,
            "f1_weighted": 0.654838
          },
          {
            "accuracy": 0.641113,
            "f1": 0.63178,
            "f1_weighted": 0.631789
          },
          {
            "accuracy": 0.598145,
            "f1": 0.594676,
            "f1_weighted": 0.594708
          },
          {
            "accuracy": 0.644043,
            "f1": 0.632416,
            "f1_weighted": 0.632418
          },
          {
            "accuracy": 0.571289,
            "f1": 0.574934,
            "f1_weighted": 0.574913
          },
          {
            "accuracy": 0.646484,
            "f1": 0.640612,
            "f1_weighted": 0.640667
          }
        ],
        "main_score": 0.627393,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ]
      }
    ]
  },
  "evaluation_time": 3.4404194355010986,
  "kg_co2_emissions": null
}